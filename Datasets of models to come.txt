WIP by George Walker (@georgejrjrjr, georgewalkeriv@gmail.com, @ghw.01 on Signal)
What is this?
These are my pretraining dataset notes, updated as I continue to think about this problem in public. It began as a speculative answer to a friend’s question: “How did Mistral do it?” i.e., create such a potent, small model.


The earliest versions of this document attracted some unexpected attention on twitter, so I expanded them to include AI dataset history, and an outline of what a high quality data curation pipeline looks like, and an overview of why this might be important to those of us working to understand, improve, and build on openly trained models in public.


I was not the only person who thinks it is high time to break the data quality bottleneck in AI:


In late January, this document seeded a project to build out large, clean, diverse datasets for openly training large language models.


Until we have more public affordances for collaboration, here’s how you can help:
* If you see an error or omission, correct it.
* If you or someone you know is working on open pre-training datasets, let’s coordinate.
* If you’re dying to fund this effort or lend our teams some compute, we won’t say no.
Should I read this if I want to train a chatbot?
Perhaps not.

This document is focused on crafting pre-training datasets, i.e., the large scale datasets for training base models. If your interest is in datasets for fine-tuning base models into chat bots or other tools, your time may be better spent following @jd_pressman’s work on RetroInstruct, lurking in the @nousresearch discord, examining the datasets @migtissera drops occasionally, and experimenting with pipelines for building synthetic datasets, such as MiniHF and DSPy.
I ain’t gonna read all that
If this document is too long or technical, you might prefer this excellent thread from Michael Edward Johnson, OpenAI engineer James Betker’s micro-famous blog post, and this podcast excerpt with Tri Dao. These all convey the central thesis of this document clearly and succinctly: language model quality is limited by the quality of their training data.
________________
Why craft pre-training datasets? I don’t have a supercomputer!
The open source AI community, infamously termed the ‘GPU Poor’ in a sneering semianalysis post, are by definition not positioned to train models near the frontier of model performance.

This capability gap has led many to assume that pre-training datasets are irrelevant to anyone without the funding to rent a supercomputer for months. In my view, this is a strategic error.


Data quality is where the Open Source AI community can dramatically improve model performance and interpretability, while furthering open science, because data quality is the primary bound on open model performance.


Training models is expensive and glamorous, yet the gains are ephemeral. While the lifespan of a state of the art model is measured in weeks or months, datasets provide intelligence yields for years. Check the publication dates on your favorite language model benchmarks! Even if crowdfund large language models were feasible, there would be little point while well-funded tech companies and academic labs are eager to do it for us. Crowdsourcing compution for training is similarly inefficient and ineffective, because model training is constrained by the speed at which GPUs communicate during a training run. At this time, all the consumer graphics cards on the Internet could not compete with a mere thousand GPUs in a supercomputer.


Data cleaning, data curation, and data synthesis do not have this problem: dataset creation is a series of (mostly) parallel operations. This makes dataset creation perfectly suited to distributed computation, as one finds on AI blockchains. We can build good datasets together.


The easier a frontier model is to train, the more competitive the open source AI space will become. Currently, model training is a turn-key affair –Databricks, Together.ai, Cerebras, and others are happy to handle supercomputer logistics for a fee. High performance inference providers abound. Architectural innovation is largely happening in public. Assembling superlative datasets is the only moat centralized AI companies have left. Put another way:


Publishing superlative datasets moves the bottleneck on the creation of frontier models from a ‘data limited’ regime, in which only a few labs have access to the requisite datasets and the expertise to produce them, to ‘checkbook limited’ regime, wherein any funder can create a frontier model. A ‘Checkbook limited’ regime will make it easy, and so more common, for funded interests to create models that anyone can use, replicate, interpret, and build upon.
 
The path to open AI near the frontier runs through the creation of open source training data.
________________
It’s the data
Mistral raised >$100M on a memo, securing an instantaneous unicorn valuation on a central premise: only a few labs know how to train frontier models efficiently. From the memo:


“We believe that most of the value in the emerging generative AI market will be located
in the hard-to-make technology, i.e. the generative models themselves […] The second important barrier lies in the difficulty to assemble an experienced team, something that
mistral.ai will be in a unique position of doing […] All major actors are currently US-based [...] This is a major geopolitical issue” [ED: emphasis mine].


This seemed true enough at the time. After all, most commentators were stunned by the quality of Mistral-7B’s output upon release, like many had been incredulous about the little noticed rumor from this author (later confirmed by Microsoft) that ChatGPT-3.5-Turbo was a mere 20 Billion parameters[1].


Open source data hero Ben Anderson, creator of the open source data cleaning package Galactic, was among the first to publicly explain the cryptic subtext behind Mistral’s raise (and the explanation for 3.5-Turbo’s rumored efficiency):  


“Few know how to train efficient models” meant “Few know how to craft informative datasets.”


Two months later, this central insight was rocketed to common knowledge with an Elon tweet, bringing attention to an obscure blog post from OpenAI engineer James Betker, The “it” in AI models is the dataset:


Trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point…model behavior is not determined by architecture, hyperparameters, or optimizer choices. It’s determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery [sic] compute to approximating that dataset.


Then, when you refer to “Lambda”, “ChatGPT”, “Bard”, or “Claude” then, it’s not the model weights that you are referring to. It’s the dataset.


Chinese labs had already taken note: Alibaba released a data distillation framework and accompanying paper, Data Juicer, to create the datasets for their Qwen model family.


Around the same time, DeepSeek-67B was released, rocketing a previously-obscure quantitative finance team to generative AI prominence.  


Like Mistral, DeepSeek-67B’s architecture is very similar to Llama 2. Their training budget was slightly less than Llama 2 70B. And the model is bilingual, meaning fewer of the tokens it saw in training were in English. And at the risk of stating the obvious: the DeepSeek models come from a team that must overcome increasingly restrictive GPU embargoes imposed by the US State Department. In spite of all this, DeepSeek-67B exceeds Llama 2 70B by most English language benchmarks, and predictably dominated in Chinese. How did they do it? Data curation. They followed up on their data-centric gains with subsequent leading releases of DeepSeekCoder and DeepSeekMath.   


Refreshingly, the Chinese labs DeepSeek, Alibaba Cloud, and OpenBMB are Building in Public to a much greater degree than their leading western counterparts. Even so, they are typically not generally releasing their datasets[2], leaving much work yet to be released in the open.


Thus, the central claims that drove Mistral’s instantaneous unicorn valuation are being challenged:
1. The central importance of data quality is becoming widely known.
2. Chinese labs like DeepSeek, Alibaba, and OpenBMB have used data curation to take positions on the frontier of permissive use models.
3. Academics like Tri Dao are pivoting their focus where it belongs: on the data.


The remaining work lies in expanding and applying our knowledge of data quality at immense scale, and rendering the resulting datasets open for all to use and study.
________________


A brief history of language model data composition
The first modern LLM was Jeremy Howard’s ULMFit. ULMFit was trained on Wikipedia. OpenAI’s first language model, GPT, was trained on a corpus of books. GPT-2 was trained on the “WebText” corpus, consisting of text scraped from websites linked on Reddit. GPT-3 expanded this dataset with web text sourced from Common Crawl, a free corpus of over 240 billion web pages collected over 16 years. As the data volume requirements of frontier models increased, data selectivity decreased, and Common Crawl became the cornerstone of every subsequent large model training effort of note.


EleutherAI formed in 2020 to publicly replicate GPT-3, and so its dataset. That dataset, termed The Pile, is a collection of text corpora including web text, books, patents, papers, and parliamentary proceedings. The Pile was the open standard in language model datasets for years after its creation, attracting significant academic, corporate, and hobbyist attention. This flurry of work taught us that human intuition for what constitutes ‘high quality’ text does not always translate into high model performance.


Efforts to optimize The Pile for language model training converged on a surprising finding: high proportions of web text (and so lower proportions of everything else in The Pile) made models smarter, all else being equal. This was counterintuitive to many, since the median quality level of web text pales in comparison to hand-picked corpora. It was far from obvious why web text would be better for model training than academic papers (to take one example). The answer seems to be diversity: web text contains nearly every conceivable usage of language.


In May of 2022, Andrej Karpathy tweeted:


Large, clean, diverse data. The 3 pillars of a good dataset.


With 240 billion web pages, Common Crawl yields ample data diversity and size. For instance, when together.ai extracted and de-duplicated a large fraction of CommonCrawl, they produced over 100T tokens in only five languages, with 20T tokens of just English after deduplication and filtering. For reference, 20T tokens is ten times more data than was used to train the Llama 2 family of permissive use models from Meta. This bounty of text prompts two questions:
1. How does one best filter text for high quality text without adversely impacting data diversity?
2. And how much does filtering for good data really matter?


Early web text quality filtering results in the language modeling literature can be summarized as, “Filtering out duplicates and noisy text is somewhat helpful”. The central importance of dataset quality only began to be recognized with the release of the Beyond Neural Scaling Laws paper from Meta. Meta found that selecting the most informative data radically improves the performance of deep learning models, given an accurate measure of data informativeness.
________________


Take a moment with these jaw dropping charts from the Beyond Neural Scaling Laws paper:


                               


On the left, we see that training on more informative data dramatically improves the efficiency of model training, as best summarized by leading deep learning architecture innovator Tri Dao, creator of Flash Attention, Flash Attention 2, and Mamba:


Ultimately it's about data. If you look at the scaling law curve,[3] different architectures would generally have the same slope, they're just different offset. It seems like the only thing that changes the slope is the data quality.


On the right: ninety percent of the data is actively harmful to the performance of this model! Training on uninformative data is worse than useless. Training on uninformative data leads to artificial brain damage. With apologies for the repetition: the model charted does not reach peak performance until and unless the least informative ~90% of the training set has been omitted! 


These were tantalizing findings, but Beyond Neural Scaling Laws addressed image models. At the time, it wasn’t obvious how those results would translate to language. One ‘gotcha’ from the authors’ data-pruning theory is that the usefulness of a data-pruning metric is bounded by its accuracy. Will we even find suitable data-pruning metrics for text? While there is still no perfect data-informativeness criterion for text, subsequent work answered this question in the affirmative. Some highlights:


Textbooks are All You Need from Microsoft Research showed that simply asking GPT-4 to assess the training value of code (then using those judgements to train a classifier aligned with those judgements) yielded coding benchmark scores on par with a leading coding model trained ~11x larger, with ~30x more tokens seen[4] –comparable performance with 300x less training.


SemDeDup from Meta showed training efficiency improvements (as measured on benchmarks) through semantic deduplication.


D4 from Meta expanded on SemDeDup, adding SSL Prototype Filtering[5] to the recipe, yielding better downstream task performance than SemDeDup alone.


DoReMi from Meta optimized presented a method for optimally sampling data from different text corpora, yielding models that modeled every corpora better, even those which were ‘downsampled’, (proportionally reduced) in the training set.


DoGE from EPFL’s Olivia Simin Fan improved on DoReMi, yielding a more stable, performant, and effective domain re-weighting method, while exploring its application to curriculum learning.


DeepSeekMath introduced a pipeline for extracting 120B tokens of mathematical content from a 40B page subset of Common Crawl. In the paper, DeepSeek used this math corpus to train a diminutive 7B parameter model with mathematical reasoning performance approaching that of far larger frontier models (GPT-4-Turbo and Gemini 1.0 Ultra).
Addressing the Goodhart in the Room
When a benchmark becomes a target, it ceases to be a good benchmark.


The performance of data selection techniques is typically measured according to the benchmark performance of models trained on the resulting data. This can be misleading, because the deep learning space has serious measurement problems. Training on test-sets (i.e., benchmark cheating) is one issue, as aptly satirized in the Phi-CTNL paper. A subtler and more pervasive problem is ‘hill climbing’, a subtler means of ‘teaching to the test’ in effect. As Omar Khattab put it:


I see a lot of accusations around that various LMs are training on the test data.


I’m surprised not to see the more obvious claim: that collectively the field is treating these test sets as hill-climbing *validation sets* with frequent testing and result-based decision making.


This means that many labs standard refine their models according to repeated tests on the same benchmarks. Since benchmarks are not comprehensive measures of everything a language model might be asked to do, this often leads to models that are overly optimized for performance on synthetic tasks, and underperform for other use cases.


DsDm from MIT CSAIL is a data selection paper targeting benchmarks explicitly, claiming a ~2x gain in ‘overall’ training efficiency. However, this claim hinges upon the idea that the benchmarks they targeted were “representative of standard LM problems.” This is dubious at best, because every use of language is a valid application of a language model!


When a dataset selection methodologies like those found in Textbooks are All You Need and DsDm[6] show outsized performance gains on benchmarks without concomitant gains in modeling large text corpora, we should be wary, and soberly ponder:


Are the models actually getting smarter? Or are they merely seeing a higher proportion of benchmark-relevant text? Are there benchmarks which are, as the DsDm authors claim, “representative of standard LM problems?”


I see no evidence that ‘standard LM problem’ is a sensible category, nor am I aware of any principled reason to suspect that it should be. Even if we assume such a category exists, it seems unlikely that humanity has discovered a representative sample. It is even more unlikely that a representative sample is publicly available. OpenAI would not be lavishing million dollar salaries on model evaluation creators if public benchmarks like MMLU, BigBench Hard, MATH, HumanEval, and Winogrande were sufficient for the purpose.


There’s even cause to wonder whether OpenAI is Goodharting their own models, much like Google has appeared to Goodhart search to the brink of unusability[7]. Researchers who are permitted to prompt GPT-4-Base reliably report that ChatGPT-4 is significantly worse on their uncommon, sometimes bizarre, tasks relative to the base model. I have only had one brief opportunity to play with GPT-4-Base, but that experience aligned with other reports: when well prompted, it is stunningly insightful relative to OpenAI’s publicly available models.


One benchmark that cannot be easily gamed is perplexity on large text corpora. This is consistent with Marcus Hutter’s parsimonious view that compression is a measure of intelligence, which seems to be holding up. Perplexity on large corpora is not a reliable measure of models trained on unknown datasets, or across different learning architectures, but it is simple, principled, and predictive of downstream task performance.  


Google DeepMind researchers recently found a metric[8] that is even more tightly correlated with downstream performance than perplexity alone (see table on the right). That said, their metric did not impact the ranking of models on downstream tasks, so experimentation will be needed to determine where and when targeting HB instead of loss will be helpful.


There are probably limits to how far perplexity and median Hurst parameters on unfiltered web text can take us in evaluation, because unfiltered web data includes reams of redundant, duplicative, and uninformative data. This suggests that measuring average perplexity on deduplicated, filtered web data may be better. Further, the available benchmarks, though limited, do seem to capture some kinds of valuable skills, and there is probably a place for data curation and synthesis methods that improve performance in particular domains, like mathematical reasoning. However, the remainder of this document assumes that generating all kinds of text are valid language model applications, and focuses on methods which have either improve general language modeling ability, or improve task performance without compromising general language modeling ability. Any exceptions to this standard must be principled and carefully considered, lest we Goodhart ourselves into mediocrity.
________________
Building Informative Training Datasets in Practice
Overview


Language model training sets consist primarily of text from the web, which is sorted and refined in a multi-stage refinement ‘pipeline’. The details of leading web text pipelines differ in their particulars, and some are closely guarded secrets, but tend to be some subset of the following:


Web Collection is a process of scraping websites from the Internet. This is typically already done by an upstream source, such as Common Crawl, which is freely available and immense. All dataset creators must do is download, decompress, and parse these pre-existing archives.


Text Extraction software pulls out the ‘main body’ of the text of each website, much like the “Reader Mode” in your favorite web browser.


Language identification tools determine the primary language in which a webpage is written, and produce ‘monolingual corpora’, i.e., groups of documents sorted by language.


Deduplication stages remove redundant content at the granularity of pages, paragraphs, or passages. Deduplication improves training efficiency, while also reducing the undesirable tendency of language models to repeat themselves repeat themselves.


Filtering removes uninformative and unwanted text, at the granularity of passages, documents, or entire data domains. Filtering combines simple, cheap techniques like counting the statistics of words and symbols, with text classifiers trained with human or large language model supervision.

Data Domain Creation organizes documents into ‘data domains’, i.e., groups of related documents. This can be done via data source (like the web domain from which it was crawled), or into semantically related ‘clusters’, i.e., groups of documents found to be similar in meaning.


Domain weighting determines how much of each data domain to sample, i.e., include, when creating a training set. This improves training efficiency in the general case, by preferentially sampling from difficult to learn domains, while decreasing the proportion of training data dedicated to simple, easy domains.


Synthetic data enrichment is used to add instruction data to data domains, so models can more readily learn to follow instructions for common language model tasks like chat, document summarization, and evaluation.


Training batch creation assembles text data into consistently sized groups, i.e., training batches for efficient model training. Recent findings indicate that models learn better when the training batches consist of semantically related text, i.e., when training batches are topical.
Collection
Common Crawl is all most of what you need
Every large language model training set, open and proprietary, consists primarily of text gleaned from websites.


The most popular source for archived websites is Common Crawl, from the non-profit Common Crawl Foundation. Common Crawl is the model training standar because it is permissively licensed, up-to-date, and gigantic: 240 billion pages and counting. It is also the most ethically unimpeachable source for web text: every site in Common Crawl was obtained consensually, in accordance with robots.txt files, which specify whether a web site owner permits automated collection of their content for ‘robots’ like search engines and language models to process.

The Common Crawl Foundation graciously makes their web archives available as both Web Archive (WARC), which contain the entire content of a website, and Web Text (WET) files, which have already undergone text extraction. These rich resources are provided free of charge on Amazon S3 (in partnership with Amazon Open Data), and mirrored by Google on Google Cloud.


There are other large sources of human generated text that many labs employ for training, such as books from shadow libraries, audio transcription from youtube, ebooks purchased from the Kindle store, and the social media data. This document focuses on consensually collected web text from Common Crawl, as the alternatives are ethically debatable and legally dicey to use and distribute.


Web text may or may not prove to be totally sufficient for any given purpose, but it does appear to be necessary, given what is established in the open literature and what we have gleaned from chatting up researchers at frontier labs. There is evidence that training on books is also helpful for model training. Our decision not to include copyrighted books without permission is a potential limitation. However, this gap is soon to be filled by a not-yet-announced effort to compile a large archive of books which are legally permissible to distribute and use for training.


Collecting web pages and preparing them for text extraction consists of two simple steps:
1. Download the Web Archive (WARC) files in which Common Crawl Foundation makes their archive available. 
2. Remove the HTML content from the WARC file, to pass off to the text extraction library.


Fortunately for dataset creators, these collection steps have been well optimized in existing, mature open source software –notably FastWARC for decompressing and parsing WARC files.
________________
Text extraction
Leave the tags. Take the text.
Text extraction tools take the HTML formatted content from web pages, and saves the ‘main body’ content as plain text. Like the “Reader Mode” one finds in modern web browsers, the most thorough text extraction libraries remove navigation bars, advertisements, and other textual intrusions into the main content of a web page. Clean extraction prevents language models from injecting such undesirable content into its output.


Existing text extraction libraries vary in the quality of their output, and their performance. Tools which produce ‘cleaner’ output often require more computation, and so time, whereas the fastest libraries tend to be less selective about their output.


The Common Crawl Foundation uses one of these ‘quick-and-dirty’ libraries in its own text extraction pipeline, which creates the “Web Text” (WET) files. These WET are source of text for the largest extant web text training corpora, Allen Institute for AI’s Dolma and Together.AI’s RedPajama-Data-v2. Since WET provided by Common Crawl are already extracted text, using them is an economical option. And yet, it is also a real limitation of prior work, and an opportunity to do better.


The most accurate text extraction libraries available at time of writing are NeuScraper, which was recently released, and Trifilatura, which is an older and more mature codebase. Trafilatura uses an array of hand-written heuristics to identify the main body content of web pages, whereas NeuScraper uses a small machine learning model to do the same.


The efficiency of code used in data extraction is very important, because Common Crawl is extremely big data. Even simple text processing operations instantaneous in most normal circumstances incur non-trivial cost at this scale. Running Trafilatura over all of Common Crawl, for instance, would cost hundreds of thousands of dollars on Amazon EC2. As such, one of the products of this project is an optimized implementation of either NeuScraper or Trafilatura[a] in safe Rust, and integrating it into Meta’s cc_net pipeline (used for language identification).
Language Identification


Meta’s cc_net is the leading tool for correctly identifying the language of extracted web text, and grouping that text into monolingual bins. A principle advantage of cc_net over other language identification software is deduplicating paragraphs across web pages before language identification takes place. This improves accuracy by removing any remaining English ‘boilerplate’ text not caught by the text extractor from web pages in other languages.


The language identification model included with cc_net was state of the art when it was released, but subsequent open source work has yielded models which identify more languages. We are evaluating these language ID models for inclusion in our pipeline.
Deduplication
How do I deduplicate thee? Let me count the ways.
Many deduplication methods for web text are useful in curating data for efficient training, while reducing the tendency of language models to repeat themselves repeat themselves. They are:
1. Line-based deduplication. This removes approximately 70% of web text (cf. cc_net, section 4.2[9]), much of it undesirable boilerplate[10]. 
2. URL-based deduplication prevents the same website from recurring in the training set, provided it is located at the same URL. Working backwards in time from the most recent slice of Common Crawl collects the latest available version of each website.
3. Exact deduplication with a Bloom filter prevents the same document from recurring in the training set, even if they occur at different locations.
4. Exact substring deduplication[11] removes (or masks) long n-grams that appear in multiple documents.
5. MinHash and Locality Sensitive Hashing provide sparse and performant means of eliminating near-duplicates without requiring document vectors, by calculating approximate Jaccard similarity.
6. SemDeDup removes documents according to the cosine similarity of their document vectors, which eliminates semantically similar, non-identical documents.


Line-based deduplication, URL-based deduplication, Bloom-filter deduplication, and MinHash/LSH deduplication are all incorporated in together.ai’s Redpajama-Data-v2 pipeline and dataset[12].


Very aggressive near-deduplication with MinHash/LSH filtering and semantic deduplication can sometimes improve benchmark scores at the cost of higher perplexity on large web corpora[13]. Meta’s D4 paper presents, in my view, the most conservative and robust recipe yet published for combining line-based, exact, minhash, and semantic deduplication. The thresholds they selected for each of these methods improves (or only very slightly harms) downstream model perplexity on large web corpora. This appears to be an intentional design choice on Meta’s part[14]. As the D4 paper notes, there may yet be better thresholds to find, but this is a sensible baseline and starting point for deduplication.
Quality Filtering
Goldilocks was a data engineer
Training data filtering is a process of triage.


“Easy” tokens, such as simplistic, repetitive text, and web ‘boilerplate’ can’t teach a model anything it won’t learn from other sources. At best, they are a waste of training compute and model capacity. At worst, they can actively hurt performance.


Undesirably “Hard” data, like text extraction artifacts, ciphertext, and hashes can’t be learned by the model at all. All else being equal, models get smarter when these are filtered out.


Data which is neither too easy to be worth training upon, nor too hard to be possible for a model to learn, is ‘Just right’.
Heuristic Quality Filtering
Text quality heuristic filtering uses computationally inexpensive indicators of ‘Good text’. They boil down to counting things, and calculating ratios[15].  Many prior datasets and academic work on filtering Common Crawl used these heuristics as strict filtering criteria. Unfortunately, because none of these heuristics is especially accurate, this risks disposing of good training data, and imposing distributional bounds on the training set that can hurt a model’s versatility.


The most comprehensive Common Crawl derived dataset to date, RedPajama-Data-v2, took a more systematic approach to these quality heuristics. Instead of choosing static filtering thresholds, the RPv2 pipeline calculates nearly every simple text quality heuristic in common usage, and appends them as document annotations. This permits these rough heuristics to be aggregated into a more accurate metric for quality than is possible with static thresholds.


We are building on the approach taken in RPv2, re-using the quality heuristics they have compiled, adding some we have under development, and aggregating them into a single p(good) metric with tabular machine learning overseen by human quality assessment, and validated with proxy model training.
Classifier Based Filtering
Taking out the garbage
Some classes of text found in abundance on the web, like search engine optimization pages, advertisements, and phishing pages for malware distribution, are plainly undesirable for a model to spend much compute learning. Others, like comments left on pornographic films, will be undesirable for a large proportion of downstream model trainers. For these, we intend to train low-overhead classifiers with FastText to detect and label or remove them from training sets.


Perplexity Filtering
Training datasets produced with cc_net, such RedPajama-Data-v2, Dolma, and LLaMa’s training set, use the perplexity of a small language model trained on Wikipedia as a heuristic for text quality. Low perplexity data is kept, high perplexity data is removed. The intuition behind this approach is that Wikipedia is representative of well-formed text, and so lower perplexity scores on a Wikipedia-trained language model is indicative of high text quality.


The use of wikipedia as a representative source of clean prose is an understandable choice, because Wikipedia is a large, multilingual, and topically diverse corpus. However, there exist many human applications for text whose distribution does not appear in wikipedia. While this is effective at disposing of certain kinds of noisy text, it also removes a large proportion of desirable text that simply doesn’t read like Wikipedia. This places undesirable distributional constraints on training sets filtered thusly.


Some knowledgeable commentators I have spoken with have expressed the belief that models trained on perplexity-filtered data appear to be overly constrained in their output. Paraphrasing an AI researcher hailing from the Cyborgist Discord: “Prompt code-davinci-002[16] with fucky schizo text with weird capitalization, and it gives you fucky schizo text with weird capitalization. Give the same prompt to LLaMa, it can’t do it, it comes out sounding like wikipedia.” This illustrates the tension between filtering out true noise the model cannot learn, with the risk of inadvertently imposing a particular linguistic style upon downstream users.


Further, Wikipedia-grounded textual filtering, like ‘bad word’ filtering, adversely impacts the cultural representation in training datasets. Corporate model funders frequently filter out words they do not wish their models to utter. While this is an understandable concern from a corporate public relations standpoint, it is not noise in the sense we mean here. Such filtering has marked costs in terms of cultural representation. For instance, one cannot filter out edgy language from a training set without also erasing the cultural products of groups that use them. Ironically, a model precluded from learning language unpalatable to Google will be useless to a lyricist writing a rap. Such linguistic discrimination is an antipattern to be avoided when constructing training-sets for general purpose models.


We believe language models should support the linguistic stylistic choices of everyone. As such, we will eschew –or at least strongly de-emphasize– the use of ‘bad word’ and Wikipedia-grounded perplexity filtering in our datasets.
 
________________
SSL Prototype Filtering: Hollowing out document clusters to remove uninformative data
Beyond Neural Scaling Laws found that images near the centroid of a cluster tend to be less informative than examples further from a cluster’s centroid. D4 replicated this finding for textual data, especially in the case where that textual data has been deduplicated.


As with the deduplication strategy outlined above, it is probably prudent to set one’s initial SSL Prototype filtering threshold just below the point at which perplexity on large web corpora is significantly harmed. In the D4 paper, operating on their semantically deduplicated cc_dedup corpus, that threshold cut 25% of the cc_dedup corpus.


The D4 paper only operated on a small slice of CommonCrawl; analysis and experimentation will likely be required to adjust these thresholds appropriately for our (much) larger corpora. Clusters become denser, and nearest neighbors come closer as data volumes increase, the pruning proportions will shift accordingly, and require validation with proxy model training.
Can LLMs filter LLM training data?
One of the simplest means of filtering training data yet tried is querying a large language model for its assessment of the value of training data. This is computationally very expensive.


In Textbooks are All You Need, GPT-4 was queried as to the educational value of permissively licensed code examples from Github. GPT-4’s judgements were used to train a random forest classifier used to filter the code corpus for the most informative 6B tokens, as determined by that process[17]. The resulting model, Phi-1, was trained on seven epochs of that 7B token corpus, and achieved stunning HumanEval and MBPP coding benchmark results. On those benchmarks, Phi-1 was competitive with StarCoder-15B, the leading open weight coding model at the time of publishing, despite being ~12x smaller, trained on ~30x fewer tokens seen, and a training set ~150x smaller than Starcoder’s. Subsequent evaluation from Meta has called into question the depth of Phi-1s real understanding of code: Phi-1 takes up the last place on CRUXEval, which assesses a model’s ability to predict the inputs and outputs of functions.


The subsequent Phi-1.5-web, Phi-1.5-webonly (from Textbooks are All You Need II) and Phi-2 models incorporated filtered web data from RefinedWeb, in addition to filtered code, but gave scant details about how this filtering was done. For instance, the prompts used with GPT-4 were not given. Phi-1.5-webonly (which, as the name suggests, was trained solely on web data) showed modestly impressive results on natural language benchmarks for its size and training budget, but not incredible results: a bit better than GPT-2. This calls into question whether this method is effective enough to justify the expense. Unhelpfully, Microsoft Research has disclosed fewer details about the Phi models training data with each subsequent release.


In How to Train Data-Efficient LLMs,[18] Google Deepmind researchers introduced “ASK-LLM”, which amounted to querying FLAN-T5 for its assessment of the value of LLM training data. Their headline results were nominally impressive, and the examples of data included vs. excluded in the appendices were tantalizing, but a deeper look at the paper shows that the headline results were cherry-picked. This calls the representativeness of their examples into question. For instance, ASK-LLM underperformed random sampling in some cases. Further, many of the most enticing examples of data curation given in the appendices were perhaps not representative. The appendices highlighted cases in which the larger ranking models selected highly information rich examples, yet their overall results were stronger with smaller (Base and XL) ranking models.


So it appears that language models can be constructively used to assess the value of training data, but those who know how to do this are keeping their methods secret. Thus, this is a rich domain for public experimentation, and further research will undoubtedly be forthcoming in the literature.
Filtering Personally Identifiable Information
Web data contains large amounts of personally identifiable information, such as phone numbers, addresses, social security numbers, leaked passwords, and email addresses. These are generally undesirable to include in language model training for reasons of privacy and model training efficiency. It also potentially exposes people who distribute our datasets, or deploy models trained on those datasets, to litigation from residents of jurisdictions which grant a ‘Right to be forgotten’. 


We’re taking a two-pronged approach to filtering out personally identifiable :
1. Use existing, established regex based filtering rules to randomize or simply omit PII detectable through these rules.
2. Identify and omit domains, such as ‘background check’ and voter registration sites, which exist primarily to serve personally identifiable information.
________________
Data Domain Creation


Every large corpus is rightly thought of as an assemblage of many smaller corpora.


Given a 240B document corpus like Common Crawl, how do we best break this out into meaningful groups to be filtered and mixed appropriately? 
Semi-Supervised Corpora Creation
DeepSeekMath Corpus, but for every domain of general human interest
Some information is more useful for acquiring a generally applicable world model than others. 


For instance, training on math content has been found to improve models ability to reason. DeepSeek pushed this to the limit with their DeepSeekMath model (and paper), in which they found 35.5M mathematical web pages with a pipeline, and used it to train a 6.8B parameter model that is competitive with GPT-4-Turbo and Gemini Ultra 1.0 in mathematical reasoning. Their pipeline looks like this:
  



This must be replicated in the open, since DeepSeek didn’t release their corpus or pipeline. And it’s a generally applicable model, given a seed corpus for every domain of human knowledge one wishes to capture. One increasingly popular approach is building on ontologies of human knowledge, either internal to existing LLMs (cf Synthetic Data (Almost) From Scratch), or using pre-existing ontologies like wikipedia or educational syllabi as seeds (cf. Cosmopedia, SciPhi, and LAB), to give a language model a broad-based education.
Unsupervised Clustering


________________


Finding the optimal mix of data domains
DoReMi from Meta was the first method to optimize the proportions of different domains of a textual corpus (The Pile) to improve performance on the entire corpus. They achieved lower loss on every domain in The Pile, while downweighting most of them. However, the domain weights DoReMi found to be optimal were unstable across their choice of proxy models, and were unstable across a training run.


  



Since the object level answer (found in several papers) for optimal domain weighting on The Pile and its derivatives is, approximately, “Upweight web crawls, downweight everything else” it wasn’t obvious how well this technique actually worked. DoGE: Domain Reweighting with Generalization Estimation from EPFL improved on this approach, reducing the computational requirements for domain weighting, increased the consistency of optimal domain weights across different proxy model sizes, stability across a training run, and downstream model performance, while introducing a method for targeting particular domains.


DoGE answers the question, “For each data domain in our training set, what proportion of each cluster should we sample to target <domain>.”


The paper lays out two approaches:
1. ‘Universal generalization,” in which the target domain is the entire corpus.
2. Domain-targeted, in which the target domain is distinct from the domains being re-weighted.


Domain-targeted domain weighting opened up a novel and reportedly highly effective dataset sequencing strategy: train on a dataset that targets (but does not include) a particular domain, and then finish by training on the targeted domain. One particularly alluring application of this is training set selection for coding models: DoGE targeting the Github domain of SlimPajama followed by training on that subset reduced loss on code by 36% over random sampling, and 14.6% over conventional fine-tuning.   
These are tremendous gains relative to most other data selection techniques demonstrated in the literature, which is all the more notable in light of the coarseness of the domains from which they were selecting.
Training Batch Creation
Learning on a Topical Island
Before training a language model, the training dataset is broken into batches of a standard size. Until recently, it was thought that randomly sampling data was optimal. However, Shi et al from Meta, UW, and AI2 found that grouping data into training batches of related content increases training efficiency, and so model performance, especially on reading comprehension tests. We intend to follow their lead.


Shi et al used a fairly elaborate means of creating these batches, reportedly to avoid duplicate content within a batch, which hurts performance. It’s not clear whether this is necessary when the training data has already been thoroughly deduplicated. While it may be simpler to follow their recipe and do a straightforward replication, given time and compute budget, we would like to run experiments to compare their method against simpler unsupervised clustering.


Batch size is a hyperparameter that downstream model trainers need to be able to tune. To make topical training batches flexible in size, we intend to cluster relatively small training batches (tentatively 2M tokens) hierarchically. This permits model trainers readily select training batches of 2M/4M/8M/16M/32M tokens while maintaining topical specificity.
Padding training batches with topical, synthetic, instruction-tuning data
You never get exactly the token count you want in a training batch; there’s always a bit of space left over. Full batches are obviously more efficient than a batch with space leftover, and padded with End of Sequence tokens.


Meanwhile, there’s mounting evidence that including instruct data in the pre-training sets helps model performance. This has been widely suspected to be part of Mistral’s strategy, as Mistral-7B and Mixtral-8x7B are both suspiciously good at instruction following for ‘base models’. The Qwen-1.5 model family, which is currently topping many of the open weight LLM leaderboards, appears to do this extensively. Qwen ‘base’ models can generate perfectly formatted instruction tuning data when prompted to do so. It would not be able to do this without the inclusion of instruction data into the datasets.


Thus, we are experimenting with padding topical training batches with topical instruction data. Finding the best method to produce grounded, correct synthetic data instruction data, but tentatively, we are particularly interested in trying John David Pressman’s RetroInstruct pipeline, grounded generation programs written on top of RAPTOR (for indexing and retrieval) and DSPy, and using language model masks and logit biases for domain adaptation (in line with Nonparametric Masked Language Modeling).


________________
Bringing it all back home


Strategically, the open source AI community (and so humanity) stands to benefit from dataset distillation, because money for model training is easier to come by than informed technical talent. By raising the dataset waterline, we can create a situation in which any pecunious party with a couple million bucks can put her preferred institution’s name up on the leaderboard for a couple weeks.


Without naming names, most people reading this will recall a very high profile, expensive open source model training endeavor that briefly made international headlines before fading into irrelevance and disuse almost immediately. The lab in question trained a 180B parameter model trained on 3.6T tokens, barely outperforming Llama 2 70B in spite of ~4x the training budget. 


It behooves us to make frontier model training as easy as possible, to make sure the next amply funded lab doesn’t also fumble the ball.


So let’s do it. There are three ways you can help this effort, at the moment:
* If you see an error or omission in this document, correct it.
* If you or someone you know is working on high potency, distilled, open web text corpora, connect us. There’s too much scope to tackle, with too much at stake, to duplicate effort.
* If you’re dying to help fund this or lend our teams compute, reach out.




Love,
George
________________
Cutting room floor


  
  

________________
[1] At that time; there is reason to suspect this has changed as sparse architectures have matured.
[2] An exception to this is OpenBMB, which has admirably pledged to publish the web corpus used to train MiniCPM-2.8B.
[3] Scaling laws are how much a model improves with increases in model parameters and training set size.
[4] Phi-1’s poor performance on Meta’s CRUXEval, along with community experience with the Phi family of models, suggest that this technique was not quite the magic bullet implied by these benchmarks. Phi-2 still punches above its weight, but by a smaller margin.
[5] SSL protype filtering is the technique that debuted in ‘Beyond Neural Scaling Laws’, and is described in more detail below.
[6]Arguably this point of caution also applies to the headline benchmark results in the SemDeDup and D4 papers, which were achieved at high pruning ratios, at the cost of perplexity over large web corpora.
[7] For example, boolean operators are no longer supported. When one is optimizing for the experience of the median user, one is unlikely to create a tool more capable than the median user can appreciate.
[8] 1/(bits-per-byte + median Hurst parameter)
[9] This figure is not current. The situation today may be different, insofar as early versions of cc_net employed WET files, whereas the latest versions extracted text from WARC files with warc2text.
[10] This makes downstream language identification more robust, as it eliminates English boilerplate from web pages in other languages.
[11] A fast implementation written in Rust is available here.
[12] MihHash is available at a variety of thresholds, but only for their ‘head’ and ‘middle’ perplexity buckets, whereas URL-based deduplication, Bloom filter deduplication, and line-based deduplication are conducted for the entire dataset.
[13] See “Addressing the Goodhart in the Room” for why this might not be a good idea.
[14] Though this isn’t an explicitly stated strategy in the D4 paper, it’s clear from a close read: D4’s ‘A8: Replicating Fixed Compute Results on C4’ adopts a nominally different pruning ratio for semantic deduplication, which corresponds precisely to the pruning ratio found in the SemDeDup paper (Section 5.3) to markedly improve training efficiency while only barely harming perplexity on OPT’s validation set.
[15] See Appendix J of the DSIR paper for a representative example.
[16] The last base model that OpenAI made available to the general public.
[17] They also used GPT-3.5-Turbo to create a 1B token corpus of ‘synthetic textbook-like problems,’ leading many commentators to incorrectly surmise that Phi-1 was trained on human generated textbooks.
[18] Ironically, no LLMs were featured in the paper. The largest model trained and featured in the paper was a 778M parameter T5 model. Of course, it’s extremely likely that their internal research was more comprehensive in this regard.
[a]wonder if there's a tool for compiling python to make this fast. https://www.modular.com/max/mojo seems to try to do this but i've heard people say they're all hype and no substance